training:
  epochs: 100
  batch_size: 4
  save_path: "/data/npl/Speech2Text/TASA/saves"
  train_path : "/data/npl/Speech2Text/rna/transformer_transducer/data/train_w2i.json"
  dev_path : "/data/npl/Speech2Text/rna/transformer_transducer/data/test_w2i.json"
  test_path : "/data/npl/Speech2Text/rna/transformer_transducer/data/test_w2i.json"
  vocab_path : "/data/npl/Speech2Text/rna/transformer_transducer/data/vocab_w2i.json"
  reload: False
  logg : "/data/npl/Speech2Text/TASA/logs/r_tasa.log"
  ctc_weight: 0.4
  


optim:
  type: adam
  lr: 0.001
  weight_decay: 0.0001
  decay_rate: 0.5


model:
   in_features: 672
   n_enc_layers: 12
   n_dec_layers: 6
   d_model: 256
   ff_size: 2048
   h: 4
   p_dropout: 0.1
   model_name: "transformer_transducer"


rnnt_loss:
  blank: 4
  reduction: "mean" 


