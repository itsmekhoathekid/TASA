training:
  epochs: 100
  batch_size: 16
  save_path: "/home/anhkhoa/TASA/saves"
  train_path : "/home/anhkhoa/transformer_transducer_speeQ/data/train_w2i.json"
  dev_path : "/home/anhkhoa/transformer_transducer_speeQ/data/test_w2i.json"
  test_path : "/home/anhkhoa/transformer_transducer_speeQ/data/test_w2i.json"
  vocab_path : "/home/anhkhoa/transformer_transducer_speeQ/data/vocab_w2i.json"
  reload: False
  logg : "/home/anhkhoa/TASA/logs/r_tasa.log"
  ctc_weight: 0.3
  result: "/home/anhkhoa/TASA/result.txt"
  type: "normal"


optim:
  type: adam
  lr: 0.001
  weight_decay: 0.0001
  decay_rate: 0.5

scheduler:
  lr_initial: 0.001
  n_warmup_steps: 15000

model:
  attention_type: "r_tasa"
  in_features: 640
  n_enc_layers: 6
  n_dec_layers: 3
  d_model: 256
  ff_size: 1024
  h: 4
  p_dropout: 0.1
  model_name: "tasa"

# optim:
#   type: sgd
#   lr: 0.0001
#   momentum: 0.9
#   weight_decay: 0
#   begin_to_adjust_lr: 60
#   nesterov: None
#   decay_rate: 0.5

rnnt_loss:
  blank: 5
  reduction: "mean"  


