# R-TASA Ã— SpeechTransformer (PyTorch)

PyTorch implementation of **Residual Transmitted & Aggregated Self-Attention (R-TASA)** integrated into **SpeechTransformer** for ASR.

> ğŸ“„ Zhang, Han, et al. â€œTASA: Transmitted and Aggregated Self-Attention for Speech Recognition.â€ *INTERSPEECH 2024*. [[paper]](https://www.isca-archive.org/interspeech_2024/zhang24q_interspeech.pdf)  
> ğŸ“„ Dong, Linhao, et al. â€œSpeech-Transformer: A No-Recurrence Sequence-to-Sequence Model for Speech Recognition.â€ *ICASSP 2018*. [[paper]](https://ieeexplore.ieee.org/document/8462506)

---

## ğŸ› ï¸ Install

```bash
git clone https://github.com/itsmekhoathekid/TASA.git
cd TASA
pip install -r requirements.txt
```

---

## âš™ï¸ Usage

Enable R-TASA in config:

```yaml
model:
  attention_type: "r_tasa" 
```

Train example:

```bash
python train.py --config config/r_tasa_local.yaml
```

---
